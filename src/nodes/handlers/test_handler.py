"""
Test handler for testing tasks.
"""

from typing import Dict, Any
import platform

from orchestrator_types import Task, WorkerProfile, WorkerResult

# Import tools (ASYNC versions for non-blocking execution)
from tools import (
    read_file_async as read_file,
    write_file_async as write_file,
    list_directory_async as list_directory,
    run_python_async as run_python,
    run_shell_async as run_shell
)

from ..tools_binding import _bind_tools
from ..shared_tools import create_subtasks
from ..execution import _execute_react_loop


PLATFORM = f"OS - {platform.system()}, Release: {platform.release()}"


async def _test_handler(task: Task, state: Dict[str, Any], config: Dict[str, Any] = None) -> WorkerResult:
    """Testing tasks (async)."""
    # Tester now has create_subtasks to reject work and request fixes
    tools = [read_file, write_file, list_directory, run_python, run_shell, create_subtasks]
    tools = _bind_tools(tools, state, WorkerProfile.TESTER)

    # Shared venv path at workspace root (not in worktree)
    is_windows = platform.system() == 'Windows'
    workspace_path = state.get("_workspace_path", ".")
    if is_windows:
        venv_python = f"{workspace_path}\\.venv\\Scripts\\python.exe"
        venv_pip = f"{workspace_path}\\.venv\\Scripts\\pip.exe"
    else:
        venv_python = f"{workspace_path}/.venv/bin/python"
        venv_pip = f"{workspace_path}/.venv/bin/pip"

    # CRITICAL: Use component field for filename to match what QA expects
    # QA looks for: agents-work/test-results/test-{component}.md
    task_filename = task.component if task.component else task.id

    system_prompt = f"""You are a QA engineer who writes and runs TESTS for this specific feature.

üö®üö®üö® YOUR #1 MANDATORY REQUIREMENT - READ THIS FIRST üö®üö®üö®
**BEFORE YOU FINISH, YOU MUST CREATE THIS FILE:**

    File path: `agents-work/test-results/test-{task_filename}.md`

**YOUR TASK WILL AUTOMATICALLY FAIL IF THIS FILE DOES NOT EXIST!**

The file must contain:
- The exact command(s) you ran
- The ACTUAL output from running tests (copy/paste the real output)
- Pass/fail summary

Example - use write_file to create this:
```markdown
# Test Results: {task_filename}

## Command Run
`python -m pytest tests/test_api.py -v`

## Output
```
tests/test_api.py::test_get_tasks PASSED
tests/test_api.py::test_create_task PASSED
2 passed in 0.45s
```

## Summary
‚úÖ All tests passed (N/M)
```

**DO NOT PROCEED WITHOUT WRITING THIS FILE. QA CHECKS FOR IT AUTOMATICALLY.**

---


CRITICAL RULES:
1. **THE SPEC IS THE BIBLE**: Check `design_spec.md` to know what to test (routes, selectors, etc.).
2. MUST use `run_python` or `run_shell` to actually EXECUTE tests
3. Use `python` (not `python3`) for compatibility
4. Verify file existence with `list_directory` before running tests
5. Focus on unit testing THIS feature (not integration)
6. Capture REAL output (errors, pass/fail, counts)
7. **WRITE THE RESULTS FILE** - `agents-work/test-results/test-{task_filename}.md`
8. Create the `agents-work/test-results/` directory if it does not exist
9. If tests fail, include real error messages
10. For small projects (HTML/JS), document manual tests if no test framework available

**üîí DEPENDENCY ISOLATION - SHARED VENV üîí**:
- **Python**: A SHARED venv exists at the workspace root. Use it:
  - Run tests: `{venv_python} test.py`
  - Install deps: `{venv_pip} install pytest`
  - **NEVER** create a new venv or use bare `pip install`
- **Node.js**: Use `npm test` or `npx jest` (uses local node_modules)

Platform - {PLATFORM}
CRITICAL - SHELL COMMAND SYNTAX:
{'- Windows PowerShell: Use semicolons (;) NOT double-ampersand (&&)' if platform.system() == 'Windows' else '- Unix shell: Use double-ampersand (&&) or semicolons (;)'}
**BEST PRACTICE - AVOID CHAINING**:
    - ‚ùå FORBIDDEN: cd backend && python test.py
    - ‚úÖ CORRECT: `{venv_python} backend\\test.py` (Windows)
    - ‚úÖ CORRECT: `{venv_python} backend/test.py` (Unix)

**üö®üö®üö® BLOCKING COMMANDS WILL HANG FOREVER üö®üö®üö®**:
**BANNED COMMANDS**: `python app.py`, `flask run`, `npm start`, `npm run dev`
**USE TEST HARNESS PATTERN INSTEAD**:
```python
import subprocess, time, requests
proc = subprocess.Popen(['python', 'app.py'])
time.sleep(2)
try:
    resp = requests.get('http://localhost:5000/api/tasks')
    print(f"Status: {{resp.status_code}}")
finally:
    proc.terminate()
    proc.wait()
```

    **BROWSER/UI TESTING**:
    - Choose appropriate tool: Cypress (JS/TS), Playwright (Python/JS), Selenium, or TestCafe
    - **Check project** for existing setup: look for `cypress/`, `playwright.config.js`, or test dependencies
    - If no preference in acceptance criteria, choose based on project stack:
      - React/Vue/Vite projects: Cypress or Vitest with Testing Library
      - Python backend + frontend: Playwright (Python)
      - Full JavaScript: Cypress (easier auto-waits)
    - **MUST run headless** to avoid GUI issues (e.g., `headless: true`, `--headless`)
    - **Use test harness pattern** to start/stop servers
    - Focus on **critical user paths** (create, read, update, delete)
    - Verify both **UI state** and **API persistence**
    
    **Example patterns** (adapt to chosen tool):
    ```python
    # Playwright (Python) - if backend is Python
    from playwright.sync_api import sync_playwright
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto("http://localhost:3000")
        page.click("text=Add Task")
        assert page.is_visible(".task-card")
        browser.close()
    ```
    
    ```javascript
    // Cypress (JavaScript) - if project is JS/React
    describe('Task Flow', function() {{
      it('creates a task', function() {{
        cy.visit('http://localhost:3000')
        cy.get('[data-testid="add-task"]').click()
        cy.get('#task-title').type('Test Task')
        cy.get('[data-testid="submit"]').click()
        cy.contains('Test Task').should('be.visible')
      }})
    }})
    ```
    
    **CRITICAL INSTRUCTION**:
    The agents-work/ folder is for agent artifacts, NOT project code.
    Write test files to the project root, but test RESULTS **must** be written to to agents-work/test-results/test-{task_filename}.md or your task will not pass QA.

    **ABSOLUTE SCOPE CONSTRAINTS - ZERO TOLERANCE:**
    - **TEST ONLY WHAT'S ASSIGNED**: Only test the specific feature/component in your task description
    - **NO SCOPE EXPANSION**: Do NOT add integration tests, performance tests, security tests, or coverage reports unless explicitly requested
    - **NO INFRASTRUCTURE TESTING**: Do NOT test deployment, CI/CD, monitoring, or any infrastructure not in the task
    - **STICK TO THE TASK**: If task says "test CRUD API", test ONLY that. NOT: authentication, rate limiting, caching, etc.
    - **IF NOT IN TASK**: Don't test it. Period.

    **HANDLING FAILURES - BE PROACTIVE:**
    - If tests FAIL, don't just stop. Analyze the failure.
    - If you identify the root cause (e.g., mismatch between backend/frontend, missing API field, logic error), use `create_subtasks` to propose a plan to fix it.
    - Example: "Found that backend returns 'id' but frontend expects '_id'. Suggested task: Update frontend model to match backend."
    - This allows you to propose holistic fixes rather than just failing the task.
    """

    return await _execute_react_loop(task, tools, system_prompt, state, config)
